from __future__ import annotations
import json
import logging
import os
import time
from datetime import date, datetime
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse

import pymysql
from dotenv import load_dotenv
from selenium import webdriver
from selenium.common.exceptions import (
    NoSuchElementException,
    TimeoutException,
    WebDriverException,
)
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager


logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)
# "LinkareerCrawler" ë¡œê±° ìƒì„±
logger = logging.getLogger("LinkareerCrawler")

load_dotenv()

DEFAULT_WAIT = 12


class LinkareerCrawler:
    """
    ë§ì»¤ë¦¬ì–´(https://linkareer.com) í¬ë¡¤ë§ í´ë˜ìŠ¤.
    """

    # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê³µëª¨ì „ ëª©ë¡ í˜ì´ì§€ URLì˜ ê¸°ë³¸ í˜•íƒœ
    Newest_Url = "https://linkareer.com/list/contest?filterType=CATEGORY&orderBy_direction=DESC&orderBy_field=CREATED_AT&page="

    BASE_URL = "https://linkareer.com"

    LIST_PATH = "/list/contest"

    def __init__(
        self,
        headless: bool = True,
        wait_time: int = DEFAULT_WAIT,
        viewport: tuple = (1200, 900),
        throttle: float = 1.0,
    ):
        """
        Args:
            headless (bool): Trueì¼ ê²½ìš° ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•Šê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
            wait_time (int): ì›¹ ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ìµœëŒ€ ì‹œê°„(ì´ˆ)
            viewport (tuple): ë¸Œë¼ìš°ì € ì°½ í¬ê¸°ë¥¼ (ë„ˆë¹„, ë†’ì´) íŠœí”Œë¡œ ì„¤ì •
            throttle (float): ê° HTTP ìš”ì²­ ì‚¬ì´ì— ì¶”ê°€í•˜ëŠ” ëŒ€ê¸° ì‹œê°„(ì´ˆ)
        """
        self.headless = headless
        self.wait_time = wait_time
        self.viewport = viewport
        self.throttle = throttle
        self.driver = None

    def _make_driver(self):
        opts = Options()

        opts.binary_location = "/opt/google/chrome/google-chrome"

        opts.add_argument("--headless=new")
        opts.add_argument("--no-sandbox")
        opts.add_argument("--disable-dev-shm-usage")
        opts.add_argument("--disable-gpu")
        opts.add_argument("--disable-software-rasterizer")
        opts.add_argument("--remote-debugging-port=9222")
        opts.add_argument("--window-size=1200,900")

        prefs = {"profile.managed_default_content_settings.images": 2}
        opts.add_experimental_option("prefs", prefs)

        # ë²„ì „ ê³ ì • (driver_version)
        chrome_driver_path = ChromeDriverManager(
            driver_version="143.0.7499.40"
        ).install()

        service = Service(chrome_driver_path)

        return webdriver.Chrome(service=service, options=opts)

    def start(self):
        """ì›¹ ë“œë¼ì´ë²„ ì‹œì‘"""
        if self.driver is None:
            logger.info("Starting WebDriver")
            self.driver = self._make_driver()

    def stop(self):
        """ì›¹ ë“œë¼ì´ë²„ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•˜ê³  ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œ"""
        if self.driver:
            try:
                self.driver.quit()
            except Exception:
                pass
            self.driver = None
            logger.info("WebDriver stopped.")

    def fetch_activity_urls(self) -> List[str]:
        """
        URL ì´ë™ ì—†ì´, í˜„ì¬ í˜ì´ì§€ì˜ ë¦¬ìŠ¤íŠ¸ ì˜ì—­ì—ì„œë§Œ activity URLë“¤ì„ ì¶”ì¶œ.
        (React ë Œë”ë§ ì•ˆì •í™” í¬í•¨)
        """
        self.start()
        driver = self.driver

        wait = WebDriverWait(driver, self.wait_time)

        # 1) list-body ë¡œë”© ëŒ€ê¸°
        try:
            wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.list-body"))
            )
        except TimeoutException:
            logger.warning("Timeout waiting for list-body on current page")
            return []

        # 2) React ë Œë”ë§ ì•ˆì •í™” (anchor ê°œìˆ˜ ë³€í™” ê°ì§€)
        prev_count = -1
        stable_count = 0
        for _ in range(20):
            anchors = driver.find_elements(
                By.CSS_SELECTOR, "div.list-body a[href^='/activity/']"
            )
            curr_count = len(anchors)

            if curr_count == prev_count:
                stable_count += 1
            else:
                stable_count = 0

            if stable_count >= 3:  # 3ë²ˆ ì—°ì† ë™ì¼ â†’ ë Œë”ë§ ì™„ë£Œ
                break

            prev_count = curr_count
            time.sleep(0.2)

        # 3) anchors íŒŒì‹±
        anchors = driver.find_elements(
            By.CSS_SELECTOR, "div.list-body a[href^='/activity/']"
        )

        logger.info("Found %d anchors on current page", len(anchors))

        seen = set()
        urls = []

        for el in anchors:
            try:
                href = el.get_attribute("href")
                if not href:
                    continue

                full_url = urljoin(self.BASE_URL, href)
                if full_url not in seen:
                    seen.add(full_url)
                    urls.append(full_url)
            except Exception:
                continue

        logger.info("Found %d unique activity URLs on current page", len(urls))
        return urls

    def fetch_activity_details(self, detail_url: str) -> Optional[Dict]:
        """
        ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        activity ìƒì„¸ í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ ì„¸ë¶€ ì •ë³´ë¥¼ ì¶”ì¶œ

        Args:
            detail_url (str): ì •ë³´ë¥¼ ì¶”ì¶œí•  ìƒì„¸ í˜ì´ì§€ì˜ ì ˆëŒ€ URL.

        Returns:
            Optional[Dict]: ì¶”ì¶œëœ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬. ì‹¤íŒ¨ ì‹œ Noneì„ ë°˜í™˜.
        """
        self.start()
        driver = self.driver
        logger.info("Visiting detail page: %s", detail_url)

        # ğŸ”¥ ìƒˆ íƒ­ ì—´ê¸°
        driver.execute_script(f"window.open('{detail_url}', '_blank');")
        driver.switch_to.window(driver.window_handles[-1])

        wait = WebDriverWait(driver, self.wait_time)
        try:
            wait.until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, "header[class^='ActivityInformationHeader__']")
                )
            )
        except TimeoutException:
            logger.warning("Timeout waiting for detail page to render: %s", detail_url)
            return None

        time.sleep(self.throttle)

        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
        result = {
            "activity_title": None,
            "activity_url": None,
            "activity_category": [],
            "start_date": None,
            "end_date": None,
            "activity_img": None,
            "organization_name": None,
            "detail_url": detail_url,
            # --- ì¶”ê°€í•´ì•¼ í•˜ëŠ” í•„ë“œë“¤ ---
            "award_scale": None,
            "benefits": None,
            "additional_benefits": None,
            "target_participants": None,
            "company_type": None,
            "views": None,
        }

        # --- ê° í•„ë“œ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---

        # ì œëª© (activity_title): í—¤ë”(<header>) ì•ˆì˜ <h1> íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        try:
            # ActivityInformationHeader__ë¡œ ì‹œì‘í•˜ëŠ” classì˜ h1
            title_element = driver.find_element(
                By.CSS_SELECTOR, "header[class^='ActivityInformationHeader__'] h1"
            )
            result["activity_title"] = title_element.text.strip()
        except NoSuchElementException:
            logger.debug("Title not found on %s", detail_url)

        # í™ˆí˜ì´ì§€ URL (activity_url): 'HomepageField' í´ë˜ìŠ¤ë¡œ ì‹œì‘í•˜ëŠ” <dl> ë‚´ë¶€ì˜ <a> íƒœê·¸ì—ì„œ href ì†ì„± ì¶”ì¶œ
        try:
            home_anchor = driver.find_element(
                By.CSS_SELECTOR, "dl[class^='HomepageField__'] a"
            )
            result["activity_url"] = home_anchor.get_attribute("href")
        except NoSuchElementException:
            logger.debug("Homepage/activity_url not found on %s", detail_url)

        # ì¹´í…Œê³ ë¦¬ (activity_category): ì¹´í…Œê³ ë¦¬ ì¹© ëª©ë¡ ë‚´ë¶€ì˜ ëª¨ë“  <p> íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ '/' ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³ , í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
        try:
            category_elements = driver.find_elements(
                By.CSS_SELECTOR, "ul[class^='CategoryChipList__'] p"
            )

            categories = []
            for p_element in category_elements:
                text = p_element.text.strip()
                if text:
                    categories.append(text)  # split í•˜ì§€ ì•ŠìŒ!

            result["activity_category"] = categories

        except NoSuchElementException:
            logger.debug("Category not found on %s", detail_url)

        # ì ‘ìˆ˜ ì‹œì‘ì¼ (start_date): 'start-at' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ <span> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        try:
            result["start_date"] = driver.find_element(
                By.CSS_SELECTOR, ".start-at + span"
            ).text.strip()
        except NoSuchElementException:
            logger.debug("Start date not found on %s", detail_url)

        # ì ‘ìˆ˜ ë§ˆê°ì¼ (end_date): 'end-at' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ <span> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        try:
            result["end_date"] = driver.find_element(
                By.CSS_SELECTOR, ".end-at + span"
            ).text.strip()
        except NoSuchElementException:
            logger.debug("End date not found on %s", detail_url)

        # ëŒ€í‘œ ì´ë¯¸ì§€ (activity_img): 'card-image' í´ë˜ìŠ¤ <img> íƒœê·¸ì˜ src ì†ì„±ì„ ì¶”ì¶œ
        try:
            result["activity_img"] = driver.find_element(
                By.CSS_SELECTOR, "img.card-image"
            ).get_attribute("src")
        except NoSuchElementException:
            logger.debug("img.card-image not found, trying fallback selector.")
            try:
                poster_img = driver.find_element(By.CSS_SELECTOR, "div.poster > img")
                result["activity_img"] = poster_img.get_attribute("src")
            except NoSuchElementException:
                logger.debug("Activity image not found on %s", detail_url)

        # --- ì¶”ê°€ í•­ëª©ë“¤ ìˆ˜ì§‘ (CSS_SELECTORëŠ” ì§ì ‘ ë„£ì–´ì•¼ í•¨) ---

        try:
            # ì˜ˆ: ìƒê¸ˆ ê·œëª¨
            result["award_scale"] = driver.find_element(
                By.CSS_SELECTOR,
                "#__next > div.id-__StyledWrapper-sc-826dfe1d-0.hLmKRJ > div > main > div > div > section:nth-child(1) > div > article > div.ActivityInfomationField__StyledWrapper-sc-2edfa11d-0.bKwmrS > dl:nth-child(3) > dd",
            ).text.strip()
        except NoSuchElementException:
            pass

        try:
            # ì˜ˆ: í˜œíƒ
            result["benefits"] = driver.find_element(
                By.CSS_SELECTOR,
                "#__next > div.id-__StyledWrapper-sc-826dfe1d-0.hLmKRJ > div > main > div > div > section:nth-child(1) > div > article > div.ActivityInfomationField__StyledWrapper-sc-2edfa11d-0.bKwmrS > dl:nth-child(6) > dd",
            ).text.strip()
        except NoSuchElementException:
            pass

        try:
            # ì˜ˆ: ì¶”ê°€ í˜œíƒ
            result["additional_benefits"] = driver.find_elements(
                By.CSS_SELECTOR,
                "#__next > div.id-__StyledWrapper-sc-826dfe1d-0.hLmKRJ > div > main > div > div > section:nth-child(1) > div > article > div.ActivityInfomationField__StyledWrapper-sc-2edfa11d-0.bKwmrS > dl:nth-child(8) > dd",
            )
            # ë°°ì—´ í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ join ì²˜ë¦¬
            result["additional_benefits"] = ", ".join(
                [el.text.strip() for el in result["additional_benefits"]]
            )
        except NoSuchElementException:
            pass

        try:
            # ì˜ˆ: ì°¸ê°€ ëŒ€ìƒ
            result["target_participants"] = driver.find_element(
                By.CSS_SELECTOR,
                "#__next > div.id-__StyledWrapper-sc-826dfe1d-0.hLmKRJ > div > main > div > div > section:nth-child(1) > div > article > div.ActivityInfomationField__StyledWrapper-sc-2edfa11d-0.bKwmrS > dl:nth-child(2) > dd",
            ).text.strip()
        except NoSuchElementException:
            pass

        try:
            # ì˜ˆ: íšŒì‚¬ ìœ í˜•
            result["company_type"] = driver.find_element(
                By.CSS_SELECTOR,
                "#__next > div.id-__StyledWrapper-sc-826dfe1d-0.hLmKRJ > div > main > div > div > section:nth-child(1) > div > article > div.ActivityInfomationField__StyledWrapper-sc-2edfa11d-0.bKwmrS > dl:nth-child(1) > dd",
            ).text.strip()
        except NoSuchElementException:
            pass

        try:
            # ì¡°íšŒìˆ˜
            result["views"] = driver.find_element(
                By.CSS_SELECTOR,
                "#__next > div.id-__StyledWrapper-sc-826dfe1d-0.hLmKRJ > div > main > div > div > section:nth-child(1) > header > div > span:nth-child(2)",
            ).text.strip()
        except NoSuchElementException:
            pass

        # ì£¼ìµœ/ì£¼ê´€ (organization_name): ë‹¤ì–‘í•œ ë¼ë²¨ì„ ëŒ€ìƒìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        try:
            result["organization_name"] = driver.find_element(
                By.CSS_SELECTOR,
                "#__next > div.id-__StyledWrapper-sc-826dfe1d-0.hLmKRJ > div > main > div > div > section:nth-child(1) > div > article > header > h2",
            ).text.strip()
        except NoSuchElementException:
            pass

        # ğŸ”¥ ìƒì„¸ í˜ì´ì§€ íƒ­ ë‹«ê¸°
        driver.close()

        # ğŸ”¥ ì›ë˜ ëª©ë¡ íƒ­ìœ¼ë¡œ ëŒì•„ê°€ê¸°
        driver.switch_to.window(driver.window_handles[0])

        return result

    def _extract_organization_name(self, driver) -> Optional[str]:
        """ìƒì„¸ í˜ì´ì§€ ë‚´ ì£¼ìµœ/ì£¼ê´€ ì •ë³´ë¥¼ ì¶”ì¶œ"""
        label_candidates = ["ì£¼ìµœ", "ì£¼ê´€", "ì£¼ìµœ/ì£¼ê´€", "ì£¼ìµœ/ì£¼ê´€/í›„ì›"]
        xpaths = [
            "//dt[contains(normalize-space(.), '{label}')]/following-sibling::dd[1]",
            "//p[contains(normalize-space(.), '{label}')]/following-sibling::*[1]",
            "//span[contains(normalize-space(.), '{label}')]/following-sibling::*[1]",
        ]
        for label in label_candidates:
            for xpath in xpaths:
                try:
                    text = driver.find_element(
                        By.XPATH, xpath.format(label=label)
                    ).text.strip()
                    if text:
                        return text
                except NoSuchElementException:
                    continue
        # ì¼ë¶€ ìƒì„¸ í˜ì´ì§€ì—ì„œëŠ” ë³„ë„ì˜ ì»´í¬ë„ŒíŠ¸ class ì´ë¦„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ë¶„ì˜ ì‹œë„
        try:
            # class ì´ë¦„ì´ HostField__ ë¡œ ì‹œì‘í•˜ëŠ” dlì— ì£¼ìµœ ì •ë³´ê°€ í¬í•¨ë˜ëŠ” ê²½ìš° ì²˜ë¦¬
            dl = driver.find_element(By.CSS_SELECTOR, "dl[class^='HostField__'] dd")
            text = dl.text.strip()
            if text:
                return text
        except NoSuchElementException:
            return None
        return None

    def get_current_page(self) -> int:
        try:
            current_btn = self.driver.find_element(
                By.CSS_SELECTOR, "button.button-page-number.active-page span"
            )
            return int(current_btn.text.strip())
        except Exception:
            return 1

    def click_page_number(self, page_number: int) -> bool:
        try:
            btn = self.driver.find_element(
                By.XPATH,
                f"//button[contains(@class,'button-page-number')]/span[text()='{page_number}']/..",
            )
            btn.click()
            return True
        except Exception:
            return False

    def click_next_arrow(self) -> bool:
        try:
            next_arrow = self.driver.find_element(
                By.CSS_SELECTOR, "button.button-arrow-next:not(.Mui-disabled)"
            )
            next_arrow.click()
            return True
        except Exception:
            return False

    def go_to_next_page(self) -> bool:
        driver = self.driver

        current = self.get_current_page()
        next_page_number = current + 1

        # 1) ê°™ì€ ë¸”ë¡ ë‚´ ë²ˆí˜¸ ë²„íŠ¼ ì¡´ì¬?
        if self.click_page_number(next_page_number):
            return True

        # 2) ì—†ë‹¤ â†’ ì˜¤ë¥¸ìª½ í™”ì‚´í‘œ í´ë¦­í•˜ì—¬ ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ ì´ë™
        if self.click_next_arrow():
            # í™”ì‚´í‘œ í´ë¦­ í›„ í˜ì´ì§€ ë²ˆí˜¸ê°€ ë°”ë€” ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
            time.sleep(1)
            return True

        # 3) ë” ì´ìƒ ì´ë™ ë¶ˆê°€ â†’ ë§ˆì§€ë§‰ í˜ì´ì§€
        return False

    def wait_for_list_update(self, prev_first_url: str):
        for _ in range(20):
            try:
                first_url = self.driver.find_element(
                    By.CSS_SELECTOR, "div.list-body a[href^='/activity/']"
                ).get_attribute("href")

                if first_url != prev_first_url:
                    return True
            except:
                pass

            time.sleep(0.2)

        return False

    def crawl_pages_by_click(
        self, max_pages: int = 100, per_page_limit: Optional[int] = None
    ):
        """
        í˜ì´ì§€ ë²„íŠ¼ í´ë¦­ ê¸°ë°˜ í¬ë¡¤ë§ (React ê¸°ë°˜ í˜ì´ì§€ ì „í™˜ ì§€ì›)
        Args:
            max_pages (int): ìµœëŒ€ ëª‡ ê°œì˜ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í• ì§€
            per_page_limit (Optional[int]): í•œ í˜ì´ì§€ì—ì„œ ëª‡ ê°œì˜ ìƒì„¸ë§Œ í¬ë¡¤ë§í• ì§€ (Noneì´ë©´ ì „ì²´)
        """
        self.start()

        # -----------------------------------------
        # âœ… ìµœì´ˆ í˜ì´ì§€ ì§ì ‘ ì ‘ê·¼ (ì¤‘ìš”!!)
        # -----------------------------------------
        first_url = f"{self.Newest_Url}1"
        logger.info(f"Opening initial list page: {first_url}")
        self.driver.get(first_url)

        # React ë Œë”ë§ ê¸°ë‹¤ë¦¼
        WebDriverWait(self.driver, self.wait_time).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.list-body"))
        )
        time.sleep(1)

        collected = []

        for _ in range(max_pages):

            # -----------------------------
            # 1) í˜„ì¬ í˜ì´ì§€ì˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ URL ê°€ì ¸ì˜¤ê¸°
            # -----------------------------
            urls = self.fetch_activity_urls()  # URL?page=N ë°©ì‹ ë¯¸ì‚¬ìš©
            if not urls:
                break

            prev_first = urls[0]  # ë‹¤ìŒ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ì—¬ë¶€ íŒë‹¨ìš©

            # -----------------------------
            # 2) ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
            # -----------------------------
            limit = per_page_limit or len(urls)

            for url in urls[:limit]:
                details = self.fetch_activity_details(url)
                if details:
                    collected.append(details)

            # -----------------------------
            # 3) ë‹¤ìŒ í˜ì´ì§€ í´ë¦­ (ì—†ìœ¼ë©´ ì¢…ë£Œ)
            # -----------------------------
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_all_elements_located(
                    (By.CSS_SELECTOR, "button.button-page-number")
                )
            )

            if not self.go_to_next_page():
                break

            # -----------------------------
            # 4) ë‹¤ìŒ í˜ì´ì§€ ë¡œë”© ì•ˆì •í™” ëŒ€ê¸°
            # -----------------------------
            self.wait_for_list_update(prev_first)

        self.stop()
        return collected


def _parse_date(date_str: Optional[str]) -> Optional[date]:
    if not date_str:
        return None
    try:
        return datetime.strptime(date_str, "%Y.%m.%d").date()
    except ValueError:
        logger.warning("Cannot parse date: %s", date_str)
        return None


def _get_required_env(var_name: str) -> str:
    value = os.getenv(var_name)
    if not value:
        raise RuntimeError(f"Environment variable {var_name} is required")
    return value


def _parse_mysql_url(url: str) -> tuple[str, int, str]:
    """jdbc:mysql://host:port/db í˜•íƒœë¥¼ host, port, db ë¡œ íŒŒì‹±"""
    if url.startswith("jdbc:"):
        url = url[len("jdbc:") :]
    parsed = urlparse(url)
    host = parsed.hostname
    port = parsed.port or 3306
    database = parsed.path.lstrip("/") if parsed.path else None
    if not host or not database:
        raise RuntimeError(
            "RDS_URL must include host and database, e.g. mysql://host:3306/dbname"
        )
    return host, port, database


def persist_contests_to_rds(records: List[Dict]) -> None:
    if not records:
        logger.info("No records to persist. Skipping DB update")
        return

    host, port, database = _parse_mysql_url(_get_required_env("RDS_URL"))
    if os.getenv("RDS_PORT"):
        port = int(os.getenv("RDS_PORT"))
    user = _get_required_env("RDS_USERNAME")
    password = _get_required_env("RDS_PASSWORD")
    if os.getenv("RDS_DB_NAME"):
        database = os.getenv("RDS_DB_NAME")

    connection = pymysql.connect(
        host=host,
        port=port,
        user=user,
        password=password,
        database=database,
        charset="utf8mb4",
        autocommit=False,
    )

    with connection:
        with connection.cursor() as cursor:

            # ============================================================
            # 1ï¸âƒ£ í˜„ì¬ DBì— ì¡´ì¬í•˜ëŠ” contest ëª©ë¡ ë¡œë”©
            # ============================================================
            cursor.execute("SELECT id, name, organization_name FROM contests")
            existing_rows = cursor.fetchall()

            existing_map = {
                (row[1], row[2]): row[0]  # (name, organization_name) â†’ id
                for row in existing_rows
            }

            logger.info("Loaded %d existing contests from DB", len(existing_map))

            # ============================================================
            # 2ï¸âƒ£ INSERT or UPDATE ë¡œì§ ì ìš©
            # ============================================================
            insert_sql = """
                INSERT INTO contests (
                    categories, end_date, image_url, name, organization_name, site_url, start_date,
                    award_scale, benefits, additional_benefits, target_participants, company_type, views
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """

            update_sql = """
                UPDATE contests
                SET start_date = %s,
                    end_date   = %s,
                    views      = %s
                WHERE id = %s
            """

            insert_payloads = []
            update_payloads = []

            for record in records:
                start_date = _parse_date(record.get("start_date"))
                end_date = _parse_date(record.get("end_date"))
                image_url = record.get("activity_img")
                site_url = record.get("activity_url") or record.get("detail_url")
                title = record.get("activity_title")
                organization = record.get("organization_name") or title or "ì •ë³´ì—†ìŒ"
                categories = record.get("activity_category") or []
                category_str = ",".join(categories)
                views = int(record.get("views") or 0)

                if not (start_date and end_date and image_url and site_url and title):
                    logger.warning(
                        "Skipping invalid record: %s", record.get("detail_url")
                    )
                    continue

                key = (title, organization)

                # ====================================================
                # ì¡´ì¬ ì—¬ë¶€ ì²´í¬ â†’ UPDATE or INSERT
                # ====================================================
                if key in existing_map:
                    contest_id = existing_map[key]
                    update_payloads.append((start_date, end_date, views, contest_id))
                else:
                    insert_payloads.append(
                        (
                            category_str,
                            end_date,
                            image_url,
                            title,
                            organization,
                            site_url,
                            start_date,
                            record.get("award_scale") or "",
                            record.get("benefits") or "",
                            record.get("additional_benefits") or "",
                            record.get("target_participants") or "",
                            record.get("company_type") or "",
                            views,
                        )
                    )

            # ============================================================
            # 3ï¸âƒ£ INSERT ì‹¤í–‰
            # ============================================================
            if insert_payloads:
                cursor.executemany(insert_sql, insert_payloads)
                logger.info("Inserted %d new contests", len(insert_payloads))

            # ============================================================
            # 4ï¸âƒ£ UPDATE ì‹¤í–‰
            # ============================================================
            if update_payloads:
                cursor.executemany(update_sql, update_payloads)
                logger.info("Updated %d existing contests", len(update_payloads))

            connection.commit()


def main():
    max_pages = int(os.getenv("LINKAREER_PAGE_LIMIT", "100"))
    per_page_limit_env = os.getenv("LINKAREER_PER_PAGE_LIMIT")
    per_page_limit = int(per_page_limit_env) if per_page_limit_env else None
    headless = os.getenv("LINKAREER_HEADLESS", "true").lower() != "false"

    crawler = LinkareerCrawler(headless=headless)

    # í˜ì´ì§€ë²ˆí˜¸ í´ë¦­ ê¸°ë°˜ í¬ë¡¤ë§
    records = crawler.crawl_pages_by_click(
        max_pages=max_pages, per_page_limit=per_page_limit
    )

    logger.info("Collected %d contest detail records", len(records))

    # ë””ë²„ê·¸ìš© (DB ì“°ê¸° ìŠ¤í‚µ)
    if os.getenv("SKIP_DB_WRITE", "false").lower() == "true":
        print(json.dumps(records[:2], indent=4, ensure_ascii=False))
        return

    # RDS ì €ì¥
    persist_contests_to_rds(records)


if __name__ == "__main__":
    main()
